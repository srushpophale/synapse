# -*- coding: utf-8 -*-
"""Synapse_codemystics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iTPeIwHwF-4Kfbb9VdOhx-Y8zKJZEvYX
"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')
from zipfile import ZipFile
from pathlib import Path

import pandas as pd

# Commented out IPython magic to ensure Python compatibility.
# %pwd
path=Path("/content/drive/MyDrive/Rock identification on lunar surface/Rock identification on lunar surface.zip")

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %pwd

path=Path("/content/drive/MyDrive/Rock identification on lunar surface/Rock identification on lunar surface.zip")

with ZipFile(path,'r') as zip_file:
  [print(i) for i in zip_file.namelist()]

file_info=zip_file.getinfo('DataSet/Train Images/Train Images/Small/ground5448.png')

print(file_info.filename)
print(file_info.file_size)
print(file_info.date_time)
print(file_info.compress_size)
print(file_info.compress_type)

source_path=Path('/content/drive/MyDrive/Rock identification on lunar surface/Rock identification on lunar surface.zip')
source_path.exists()

destination_path=Path("drive/MyDrive/test")
destination_path.exists()

with ZipFile(source_path) as zip_file:
  zip_file.extractall(destination_path)

tr_l_path = '/content/drive/MyDrive/test/DataSet/train.csv'
tr_s_path = '/content/drive/MyDrive/test/DataSet/train.csv'
te_path = '/content/drive/MyDrive/test/DataSet/test.csv'

# Load CSV files into DataFrames
tr_l_df = pd.read_csv(tr_l_path)
tr_s_df = pd.read_csv(tr_s_path)
te_df = pd.read_csv(te_path)

# Display the number of records in each DataFrame
print("\n -> Number of records in train large:", len(tr_l_df))
print("\n -> Number of records in train small:", len(tr_s_df))
print("\n -> Number of records in test:", len(te_df))
tr_s_path = '/content/drive/MyDrive/test/DataSet/train.csv'
te_path = '/content/drive/MyDrive/test/DataSet/test.csv'

# Load CSV files into DataFrames
tr_l_df = pd.read_csv(tr_l_path)
tr_s_df = pd.read_csv(tr_s_path)
te_df = pd.read_csv(te_path)

# Display the number of records in each DataFrame
print("\n -> Number of records in train large:", len(tr_l_df))
print("\n -> Number of records in train small:", len(tr_s_df))
print("\n -> Number of records in test:", len(te_df))

import os

tr_l_list = os.listdir('/content/drive/MyDrive/test/DataSet/Train Images/Train Images/Large')

tr_s_list = os.listdir('/content/drive/MyDrive/test/DataSet/Train Images/Train Images/Small')

te_list = os.listdir('/content/drive/MyDrive/test/DataSet/Test Images/Test Images')

print("\n -> Number of images in train large:", len(tr_l_list))
print("\n -> Number of images in train small:", len(tr_s_list))
print("\n -> Number of images in test:", len(te_list))

import cv2
from google.colab.patches import cv2_imshow

img=cv2.imread("/content/drive/MyDrive/test/DataSet/Train Images/Train Images/Large/clean0985.png")
cv2_imshow(img)

from google.colab.patches import cv2_imshow
import cv2

# Specify the path to the directory containing the images
directory_path = "/content/drive/MyDrive/test/DataSet/Train Images/Train Images/Large/"

# Specify the filenames of the 5 images you want to display
image_filenames = ["clean0985.png", "clean0986.png", "clean0987.png", "clean0988.png", "clean0989.png"]

# Loop through the image filenames and display each image
for filename in image_filenames:
    image_path = directory_path + filename
    img = cv2.imread(image_path)

    # Display the image
    cv2_imshow(img)

df = pd.read_csv('/content/drive/MyDrive/test/DataSet/train.csv')

print("\n -> Displaying head of train dataframe.\n")

df.head()

print("\n -> Shape of train data:", df.shape)
print("\n -> Number of rows in train data:", df.shape[0])
print("\n -> Number of columns in train data:", df.shape[1])
print("\n -> Number of images in train data:", df.shape[0])
print("\n -> Number of images of class Large in train data:", df.Class.value_counts()[1])
print("\n -> Number of images of class Small in train data:", df.Class.value_counts()[0])

print("\n -> Count of Large and Small class.\n")
print(df.Class.value_counts())

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# %matplotlib inline
print("\n -> Graphically visualzing count of Large and Small class.\n")

sns.countplot(df.Class)
plt.show()

import keras

from keras.preprocessing.image import ImageDataGenerator

img_gen = ImageDataGenerator(rotation_range = 30,
                            width_shift_range = 0.1,
                            height_shift_range = 0.1,
                            rescale = 1/255,
                            zoom_range = 0.2)

print(" -> Original image from Train Large\n")

re = cv2.imread('/content/drive/MyDrive/test/DataSet/Train Images/Train Images/Large/'+tr_l_list[1])
re = cv2.cvtColor(re, cv2.COLOR_BGR2RGB)

plt.imshow(re)
plt.show()

print(" -> Generated image from Train Large\n")

re = cv2.imread('/content/drive/MyDrive/test/DataSet/Train Images/Train Images/Large/clean0001.png')
re = cv2.cvtColor(re, cv2.COLOR_BGR2RGB)

plt.imshow(re)
plt.show()

"""OBSERVATION

"""

print("\n -> Viewing number of classes in train data\n")

print(img_gen.flow_from_directory('/content/drive/MyDrive/test/DataSet/Train Images/Train Images'))

print("\n -> Shape of image:", re.shape)
print("\n -> Width of image:", re.shape[0])
print("\n -> Width of image:", re.shape[1])
print("\n -> Number of color channels of image:", re.shape[2])

input_shape = (480, 480, 3)

from keras.models import Sequential, Model

from keras import regularizers

from keras.layers import Activation, Conv2D, MaxPool2D, Dropout, Dense, Flatten

from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping

checkpoint_2 = ModelCheckpoint("lunar_model_2.h5", monitor = "train_loss", mode = "min",
                                save_best_only = True, verbose = 1)


earlystop_2 = EarlyStopping(monitor = 'train_loss', mode = "min", patience = 5,
                            verbose = 1, restore_best_weights = True)

tensorboard_2 = TensorBoard(log_dir = "drive/My Drive/Lunar Rock/lunar_graph_2",
                         histogram_freq = 0, batch_size = 500, write_graph = True,
                         write_grads = False, write_images = False, embeddings_freq = 0,
                         embeddings_layer_names = None, embeddings_metadata = None,
                         embeddings_data = None, update_freq = 'epoch')


callback_2 = [checkpoint_2, earlystop_2, tensorboard_2]

import warnings
warnings.filterwarnings('ignore')


model = Sequential()

model.add(Conv2D(filters = 128, kernel_size = (3,3), input_shape = input_shape, activation = 'relu', kernel_regularizer = regularizers.l2(0.01)))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.3))

model.add(Conv2D(filters = 64, kernel_size = (3,3), input_shape = input_shape, activation = 'relu', kernel_regularizer = regularizers.l2(0.01)))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.3))


model.add(Conv2D(filters = 64, kernel_size = (3,3), input_shape = input_shape, activation = 'relu', kernel_regularizer = regularizers.l2(0.01)))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.3))

model.add(Flatten())

model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.3))

model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

import pydot_ng as pydot
from keras.utils import plot_model
from IPython.display import Image

plot_model(model, show_shapes = True, show_layer_names = True, to_file = '/content/drive/MyDrive/test/DataSet/Train Images/Train Images/Large/clean0001.png')

Image(retina = True, filename = '/content/drive/MyDrive/test/DataSet/Train Images/Train Images/Large/clean0001.png')

model.summary()

batch_size = 16

train_img_gen = img_gen.flow_from_directory(('/content/drive/MyDrive/test/DataSet/Train Images/Train Images'),
                                           target_size = input_shape[:2],
                                           batch_size = batch_size,
                                           class_mode = 'binary')

print("\n -> Class indices.\n")

print(train_img_gen.class_indices)

import warnings
warnings.filterwarnings('ignore')

results = model.fit_generator(train_img_gen, epochs = 20, steps_per_epoch = 100, callbacks = callback_2)

print("Original small image after resizing\n")

r = cv2.imread('/content/drive/MyDrive/test/DataSet/Train Images/Train Images/Small/ground0001.png')
r = cv2.cvtColor(r, cv2.COLOR_BGR2RGB)
r = cv2.resize(r, (480, 480))
r = np.expand_dims(r, axis = 0)

test = pd.read_csv('/content/drive/MyDrive/test/DataSet/test.csv')

print("\n -> Displaying head of test data.\n")

test.head()

print("\n -> Shape of test data:", test.shape)
print("\n -> Number of rows in test data:", test.shape[0])
print("\n -> Number of columns in test data:", test.shape[1])
print("\n -> Number of images in test data:", test.shape[0])

print("First 10 images from test")
print('*'*35, '\n')

fig = plt.figure(figsize=(20,20))

read = cv2.imread('/content/drive/MyDrive/test/DataSet/Test Images/Test Images/lg 988 (1).png')
img = cv2.cvtColor(read, cv2.COLOR_BGR2RGB)
fig.add_subplot(5,2,1)
plt.imshow(img)

for i in range(2, 11):

    fig.add_subplot(5,2,i)

    read = cv2.imread('/content/drive/MyDrive/test/DataSet/Test Images/Test Images/lg 988 (1).png')
    img = cv2.cvtColor(read, cv2.COLOR_BGR2RGB)
    plt.imshow(img)

plt.show()

test_files = np.array(test['Image_File'])

test_files[:5]

import os

test_arr = []

for i in range(len(test_files)):

    file_path = os.path.join('/content/drive/MyDrive/test/DataSet/Test Images/Test Images', test_files[i])
    t = cv2.imread(file_path)
    t = cv2.cvtColor(t, cv2.COLOR_BGR2RGB)
    t = cv2.resize(t, (480, 480))
    t = np.expand_dims(t, axis = 0)
    t = np.array(t)
    test_arr.append(t)

predict = []

for i in range(len(test_arr)):

    pred = model.predict(test_arr[i])
    predict.append(pred)

test.head()

pred_list = []

for i in range(len(predict)):

    pred = int(predict[i][0][0])
    pred_list.append(pred)

print(f"Length of test_arr: {len(test_arr)}")
print(f"Length of test: {len(test)}")

print("\n -> Class indices.\n")
train_img_gen.class_indices

test['Class'] = test['Class'].apply(lambda x: 'Large' if x == 0 else 'Small')

# Load unseen images
unseen_files = ['/content/drive/MyDrive/test/DataSet/Test Images/Test Images/lg 988 (1).png']
unseen_arr = []
for file in unseen_files:
    img = cv2.imread(file)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (480, 480))
    img = np.expand_dims(img, axis=0)
    unseen_arr.append(img)

# Predict the class of unseen images
unseen_pred = []
for arr in unseen_arr:
    pred = model.predict(arr)
    unseen_pred.append(pred[0][0])

# Add predictions to a new DataFrame
unseen_df = pd.DataFrame(unseen_pred, columns=['Probability'])
unseen_df['Filename'] = unseen_files

# Set the class label based on the predicted probability
unseen_df['Class'] = unseen_df['Probability'].apply(lambda x: 'Large' if x > 0.5 else 'Small')

# Append predictions to the existing test DataFrame
test = test.append(unseen_df, ignore_index=True)

import cv2
import numpy as np

# Define the file name of the image to classify
file_name = '/content/drive/MyDrive/test/DataSet/Test Images/Test Images/lg 988 (1).png'

# Load and preprocess the image
img = cv2.imread(file_name)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img = cv2.resize(img, (480, 480))
img = np.expand_dims(img, axis=0)
img = np.expand_dims(img, axis=-1)
img = np.squeeze(img, axis=-1)

# Predict the class of the image
predictions = model.predict(img)

# Map the predicted probability to a class label
class_label = 'Small' if predictions < 0.5 else 'Large'

# Print the result
print(f'The object in the image {file_name} is {class_label}')

import cv2
import numpy as np

# Define the file name of the image to classify
file_name = '/content/drive/MyDrive/test/DataSet/Test Images/Test Images/lg 988 (126).png'

# Load and preprocess the image
img = cv2.imread(file_name)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img = cv2.resize(img, (480, 480))
img = np.expand_dims(img, axis=0)
img = np.expand_dims(img, axis=-1)
img = np.squeeze(img, axis=-1)

# Predict the class of the image
predictions = model.predict(img)

# Map the predicted probability to a class label
class_label = 'Small' if predictions < 0.5 else 'Large'

# Print the result
print(f'The object in the image {file_name} is {class_label}')

from sklearn.metrics import classification_report

# Get the true labels of the test images
true_labels = mnist.target

print(f'Length of true_labels: {len(true_labels)}')
print(f'Length of predictions: {len(predictions)}')

import tensorflow as tf
from sklearn.metrics import precision_score, recall_score, f1_score
import numpy as np

# Load the MNIST dataset
mnist = tf.keras.datasets.mnist

# Load the test images and labels
test_images, test_labels = mnist.load_data()[1]

# Normalize the test images
test_images = test_images / 255.0

# Modify the model to accept the input shape of the test images
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Make predictions on the test images
predictions = model.predict(test_images)

# Get the true labels of the test images
true_labels = test_labels

# Compute the precision, recall, and F1 score
precision = precision_score(true_labels, np.argmax(predictions, axis=1), average='weighted')
recall = recall_score(true_labels, np.argmax(predictions, axis=1), average='weighted')
f1 = f1_score(true_labels, np.argmax(predictions, axis=1), average='weighted')

# Print the precision, recall, and F1 score
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")